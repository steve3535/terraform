---
- hosts: localhost
  gather_facts: false
  vars:
         myvms:
                 - {vm_name: "LU771 - LNX - AM193 VM TEST", vm_num_sockets: 2, vm_num_cpu_per_socket: 1, vm_ram: 4096, vm_disk2_size: 102400, vm_ip: 192.168.25.248, vm_ip_prefix: 24, vm_ip_gateway: 192.168.25.1, vm_cluster: LU480, vm_subnet: "New_PROD 192.168.25.x", vm_storage: NUT_AHV_LU480_DC01_01, vm_image: RHEL8STD, vm_env: 'dev_test'}
         pc_ip: lu652
         pc_url: "https://{{ pc_ip }}:9440/api/nutanix/v3"
         machine_names: []
         inventory_dir: "{{ lookup('env','HOME') }}/infrastructure-linux/inventory/"
  vars_files:
  - "{{ lookup('env','HOME') }}/infrastructure-linux/ansible_vault/http_creds.yml"
  - "{{ lookup('env','HOME') }}/infrastructure-linux/ansible_vault/nutanix_creds.yml"

  tasks:
  - name: Save the VM names
    set_fact:
       machine_names: "{{ machine_names + [ item.vm_name|split()|first|lower() ] }}"
    loop: "{{ myvms }}"

  - name: update inventory 
    lineinfile:
      line: "{{ item.vm_name | split() | first | lower() }}"
      state: present
      path: "{{ inventory_dir }}/{{ item.vm_cluster | lower() }}"
      insertafter: "{{ item.vm_env }}"    
    loop: "{{ myvms }}"

####################### PROVISION 

  - name: lets provision
    include_role:
      name: ahv_prov
    vars:
      vm_defs: "{{ myvms }}"

  - name: add hosts 
    add_host:
      hostname: "{{ item.vm_name|split()|first|lower() }}"
      ansible_ssh_user: localadmin
      ansible_ssh_password: "{{ localadmin_password }}"
      ansible_become_password: "{{ localadmin_password }}"
      inventory_dir: "{{ inventory_dir }}"
      groups:
      - "{{ item.vm_env[0] }}"
      - "{{ item.vm_cluster|lower() }}"
      - lalux
      environ: "{{ item.vm_env|upper() }}"
    loop: "{{ myvms }}"
  - name: Wait for hosts to be reachable 
    wait_for:
      host: "{{ item.vm_name|split()|first|lower() }}"
      timeout: 120
    loop: "{{ myvms }}"

###################### SATELLITE ENROLLMENT 

- hosts:  "{{ hostvars['localhost']['machine_names']| join(',') }}"
  gather_facts: no
  become: true 
  vars_files:
  - "{{ lookup('env','HOME') }}/infrastructure-linux/ansible_vault/satellite_creds.yml"
  roles:
  - role: satellite_enrollement
    release_version: "8"
    environment_name: "{{ hostvars[inventory_hostname]['environ'] }}/CCOV-LALUX-APP-EDGE"

  tasks:
  - name: UPDATING SYSTEMS ... 
    yum:
          name: '*' 
          state: latest
 
  - name: setup application storage and filesystem
    block:
    - name: create app volume group
      lvg:  
              vg: app_vg
              pvs: /dev/sdb

    - name: create app logical volume
      lvol:
              vg: app_vg
              lv: opt
              size: 90G

    - name: create app filesystem
      filesystem:
              fstype: xfs
              dev: /dev/mapper/app_vg-opt

    - name: mount in fstab
      mount:
              path: /opt
              src: /dev/mapper/app_vg-opt
              fstype: xfs
              state: mounted
  
  - name: reboot the servers
    reboot:      
  
  environment: 
      no_proxy: lu711.lalux.local,172.22.56.10,lu714.linux.lalux.local,192.168.42.93         


############################# IDM ENROLLMENT 

- hosts:  "{{ hostvars['localhost']['machine_names']| join(',') }}"
  become: true 
  vars_files:  "{{ lookup('env','HOME') }}/infrastructure-linux/ansible_vault/idm_creds.yml" 
  vars: 
        HOME_DIR: "{{ lookup('env','HOME') }}"
        USER: "{{ lookup('env','USER') }}"
        nd_exp_pkg: "{{ lookup('env','HOME') }}/infrastructure-linux/playbooks/node_exporter-1.3.1.linux-amd64.tar.gz"
        nd_svc_file: "{{ lookup('env','HOME') }}/infrastructure-linux/playbooks/node-exporter.service"
  
  gather_facts: yes  
  
  roles: 
  - role: ipaclient
    ipaadmin_principal: admin
    ipaadmin_password: "{{ idm_password }}"
    ipaclient_domain: linux.lalux.local
    ipaclient_hostname: "{{ inventory_hostname }}.lalux.local"
    ipaclient_mkhomedir: yes
    ipaservers: lu714.linux.lalux.local
    ipaclient_force: yes
    state: present
  
  tasks:
  - name: set bash as default shell
    lineinfile:
      line: "{{ item }}"
      path: /etc/sssd/sssd.conf
      insertbefore: 'sssd'
      state: present
    loop:
      - "default_shell = /bin/bash"
      - "override_shell = /bin/bash"
  - name: restart sssd
    service:
            name: sssd
            state: restarted

######################  DEPLOY SSH PUBKEY

  - name: authorize ssh public key 
    block: 
    - name: create .ssh if not existing
      file:
        state: directory
        mode: 0700
        owner: "{{ USER }}"
        group: "{{ USER }}"
        path: "{{ HOME_DIR }}/.ssh"
    - name: deploy ssh specific pub key
      authorized_key:
            user: "{{ USER }}"
            key: "{{ lookup('file','{{ HOME_DIR }}/my_key.pub') }}"

    - name: ensure correcty possitioned access rights on /home 
      file:
              path: /home/lalux.local
              owner: root
              group: root
              mode: 0711


########### MONITORING: SETUP NODE_EXPORTER SERVICE 

  - name: ensure gtar command exists
    yum:
        name: tar
        state: present
        disablerepo: '*'
        enablerepo: rhel-8-for-x86_64-appstream-rpms,rhel-8-for-x86_64-baseos-rpms
  - name: copy and extract node_exporter on the target
    unarchive:
      src: "{{ nd_exp_pkg }}"
      dest: /opt/
  - name: copy the binary to /usr/local/bin
    copy:
      src: /opt/node_exporter-1.3.1.linux-amd64/node_exporter
      dest: /usr/local/bin/
      remote_src: yes
      mode: 0755
  - name: create a basic service
    copy:
      src: "{{ nd_svc_file }}"
      dest: /etc/systemd/system/
    notify: load_config_change
  - name: open required ports
    firewalld:
      port: 9100/tcp
      permanent: true
      immediate: yes
      state: enabled
    ignore_errors: true                                           #par le fait que le firewall nest pas active sur tous les hosts
  handlers:
  - name: load_config_change
    systemd:
      name: node-exporter.service
      daemon-reload: yes
      state: restarted
      enabled: yes
